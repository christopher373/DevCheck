{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "705039f2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# [Entropy and Mutual Information](https://people.cs.umass.edu/~elm/Teaching/Docs/mutInf.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89613d20",
   "metadata": {},
   "source": [
    "## Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d099c2",
   "metadata": {},
   "source": [
    "A function which attempts to characterize the \"unpredictability\" of a random variable. If a random variable $X$ takes on values in a set $\\mathcal{X}=\\{x_1,x_2,\\ldots,x_n\\}$, and is defined by a probability distribution $P(X)$, then we will write the entropy of the random variable as\n",
    "\n",
    "$$H(X)=-\\sum_{x\\in\\mathcal{X}}P(x)\\log P(x)$$\n",
    "\n",
    "If the log is taken to be base 2, then the entropy is expressed in *bits*. If the log is taken to be the natural log, then the entropy is expressed in *nats*."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
